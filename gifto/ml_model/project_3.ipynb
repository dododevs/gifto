{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fake users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the random seed so results are reproducible\n",
    "random.seed(42)\n",
    "\n",
    "# Number of synthetic users you want to create\n",
    "num_users = 100000\n",
    "\n",
    "# Generate user IDs (1 to num_users)\n",
    "user_ids = range(1, num_users + 1)\n",
    "\n",
    "# Generate random ages between 18 and 70\n",
    "ages = [random.randint(18, 70) for _ in range(num_users)]\n",
    "\n",
    "# Random genders (feel free to expand beyond binary)\n",
    "genders = [random.choice(['male', 'female']) for _ in range(num_users)]\n",
    "\n",
    "# Define a pool of possible hobbies\n",
    "possible_hobbies = [\n",
    "    'cars', 'tech', 'music', 'fashion', 'books',\n",
    "    'art', 'sports', 'gaming', 'cooking', 'travel'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pick a random subset of hobbies\n",
    "def pick_hobbies():\n",
    "    # Randomly choose how many hobbies this user will have (1 to 3)\n",
    "    num_hobbies = random.randint(1, 3)\n",
    "    chosen = random.sample(possible_hobbies, num_hobbies)\n",
    "    # Return them as a comma-separated string (e.g., \"cars,gaming\")\n",
    "    return \",\".join(chosen)\n",
    "\n",
    "# Create the hobbies column\n",
    "hobbies_list = [pick_hobbies() for _ in range(num_users)]\n",
    "\n",
    "# Assemble into a DataFrame\n",
    "users_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'age': ages,\n",
    "    'gender': genders,\n",
    "    'hobbies': hobbies_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age  gender                hobbies\n",
      "0        1   58    male                 sports\n",
      "1        2   25    male                 travel\n",
      "2        3   19  female         gaming,fashion\n",
      "3        4   65  female     fashion,gaming,art\n",
      "4        5   35  female   cooking,tech,fashion\n",
      "5        6   33    male          books,cooking\n",
      "6        7   32    male           books,gaming\n",
      "7        8   26    male           tech,fashion\n",
      "8        9   65  female           tech,fashion\n",
      "9       10   24    male  travel,fashion,gaming\n"
     ]
    }
   ],
   "source": [
    "# Show a sample\n",
    "print(users_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic users.csv created with 100000 users!\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "users_df.to_csv(\"users.csv\", index=False)\n",
    "print(\"Synthetic users.csv created with\", num_users, \"users!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of User-Product Interactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged products + categories (head):\n",
      "   product_id                                              title  \\\n",
      "0  B014TMV5YE  Sion Softside Expandable Roller Luggage, Black...   \n",
      "1  B07GDLCQXV  Luggage Sets Expandable PC+ABS Durable Suitcas...   \n",
      "2  B07XSCCZYG  Platinum Elite Softside Expandable Checked Lug...   \n",
      "3  B08MVFKGJM  Freeform Hardside Expandable with Double Spinn...   \n",
      "4  B01DJLKZBA  Winfield 2 Hardside Expandable Luggage with Sp...   \n",
      "\n",
      "                                              imgUrl  \\\n",
      "0  https://m.media-amazon.com/images/I/815dLQKYIY...   \n",
      "1  https://m.media-amazon.com/images/I/81bQlm7vf6...   \n",
      "2  https://m.media-amazon.com/images/I/71EA35zvJB...   \n",
      "3  https://m.media-amazon.com/images/I/91k6NYLQyI...   \n",
      "4  https://m.media-amazon.com/images/I/61NJoaZcP9...   \n",
      "\n",
      "                             productURL  stars   price  category_id  \\\n",
      "0  https://www.amazon.com/dp/B014TMV5YE    4.5  139.99          104   \n",
      "1  https://www.amazon.com/dp/B07GDLCQXV    4.5  169.99          104   \n",
      "2  https://www.amazon.com/dp/B07XSCCZYG    4.6  365.49          104   \n",
      "3  https://www.amazon.com/dp/B08MVFKGJM    4.6  291.59          104   \n",
      "4  https://www.amazon.com/dp/B01DJLKZBA    4.5  174.99          104   \n",
      "\n",
      "   isBestSeller category_name  \n",
      "0         False     Suitcases  \n",
      "1         False     Suitcases  \n",
      "2         False     Suitcases  \n",
      "3         False     Suitcases  \n",
      "4         False     Suitcases  \n",
      "Saved merged_product_with_categories.csv with 1393564 products!\n"
     ]
    }
   ],
   "source": [
    "# 1) Load users and products\n",
    "categories_df = pd.read_csv(\"amazon_categories.csv\")  # category_id, category_name\n",
    "users_df = pd.read_csv(\"users.csv\")  # user_id, age, gender, hobbies, ...\n",
    "products_df = pd.read_csv(\"amazon_products_cleaned.csv\")  # asin (or product_id), price, category_id, ...\n",
    "\n",
    "# 2) Merge categories onto products via category_id\n",
    "#    Note: 'category_id' in products_df matches 'id' in categories_df\n",
    "merged_products_df = pd.merge(\n",
    "    products_df, \n",
    "    categories_df,\n",
    "    left_on=\"category_id\", \n",
    "    right_on=\"id\",  # from categories.csv\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename for clarity: we'll keep 'category_name' but drop 'id' if redundant\n",
    "merged_products_df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# Let's rename 'asin' to 'product_id' to keep naming consistent\n",
    "merged_products_df.rename(columns={'asin': 'product_id'}, inplace=True)\n",
    "\n",
    "# Now merged_products_df has:\n",
    "# product_id, title, price, category_id, isBestSeller, category_name, ...\n",
    "print(\"Merged products + categories (head):\")\n",
    "print(merged_products_df.head(5))\n",
    "\n",
    "# Save to CSV\n",
    "merged_products_df.to_csv(\"merged_product_with_categories.csv\", index=False)\n",
    "print(\"Saved merged_product_with_categories.csv with\", len(merged_products_df), \"products!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'age', 'gender', 'hobbies'], dtype='object')\n",
      "Index(['product_id', 'title', 'imgUrl', 'productURL', 'stars', 'price',\n",
      "       'category_id', 'isBestSeller', 'category_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print the columns of the users and products dataframes\n",
    "print(users_df.columns)\n",
    "print(merged_products_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert to lists of dicts for quick random choice\n",
    "user_records = users_df.to_dict(orient=\"records\")\n",
    "product_records = merged_products_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Decide how many (user, product) interactions to generate\n",
    "num_interactions = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_like_probability(user, product):\n",
    "    \"\"\"\n",
    "    Compute probability that 'user' likes 'product',\n",
    "    using user attributes (age, gender, hobbies)\n",
    "    and product attributes (price, category_name, isBestSeller, stars, etc.).\n",
    "    \"\"\"\n",
    "    # Start with base probability\n",
    "    prob = 0.10\n",
    "\n",
    "    # User info\n",
    "    user_hobbies = user[\"hobbies\"].split(\",\")  # e.g. \"knitting,sewing\"\n",
    "    user_hobbies = [h.strip().lower() for h in user_hobbies]\n",
    "    \n",
    "    # Product info\n",
    "    category_name = str(product.get(\"category_name\", \"\")).lower()\n",
    "    price = product.get(\"price\", 0.0)\n",
    "    stars = product.get(\"stars\", 3.0)\n",
    "    is_bestseller = bool(product.get(\"isBestSeller\", False))\n",
    "\n",
    "    # 1. Hobbyâ€“category match => +30%\n",
    "    #    If the product's category_name contains a substring that appears in user hobby\n",
    "    #    e.g., \"Knitting & Crochet Supplies\" -> \"knitting & crochet supplies\"\n",
    "    #    If user hobby is \"knitting\", we detect that substring\n",
    "    for hobby in user_hobbies:\n",
    "        if hobby in category_name:\n",
    "            prob += 0.30\n",
    "            break  # stop at the first match\n",
    "\n",
    "    # 2. Price-based logic\n",
    "    if price < 20:\n",
    "        prob += 0.10  # cheaper items => more likely to try/buy\n",
    "    elif price > 100:\n",
    "        prob -= 0.10  # expensive => less likely\n",
    "\n",
    "    # 3. Star-based logic\n",
    "    if stars >= 4.5:\n",
    "        prob += 0.15\n",
    "    elif stars <= 2.5:\n",
    "        prob -= 0.15\n",
    "\n",
    "    # 4. Bestseller => +10%\n",
    "    if is_bestseller:\n",
    "        prob += 0.10\n",
    "\n",
    "    # 5. Clamp to [0, 0.90]\n",
    "    prob = max(0, min(prob, 0.90))\n",
    "\n",
    "    return prob\n",
    "\n",
    "rows = []\n",
    "for _ in range(num_interactions):\n",
    "    user = random.choice(user_records)\n",
    "    product = random.choice(product_records)\n",
    "\n",
    "    like_prob = compute_like_probability(user, product)\n",
    "    target = 1 if random.random() < like_prob else 0\n",
    "\n",
    "    rows.append({\n",
    "        \"user_id\": user[\"user_id\"],\n",
    "        \"product_id\": product[\"product_id\"],\n",
    "        \"target\": target\n",
    "    })\n",
    "\n",
    "interactions_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved user_product_interactions.csv with 100000 rows!\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates if you only want each pair once\n",
    "interactions_df.drop_duplicates(subset=[\"user_id\", \"product_id\"], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "interactions_df.to_csv(\"user_product_interactions.csv\", index=False)\n",
    "print(\"Saved user_product_interactions.csv with\", len(interactions_df), \"rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Interactions with Users and Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged user_interactions (head):\n",
      "   user_id  product_id  target  age  gender                hobbies\n",
      "0    83811  B07R9RZM7S       1   25    male     cars,gaming,sports\n",
      "1    36049  B081H1L3SB       1   48  female   music,fashion,sports\n",
      "2    96531  B07Y3ZX76D       0   27    male  gaming,fashion,sports\n",
      "3    71483  B000EVU89I       0   45  female              music,art\n",
      "4     4166  B08SHHXZDW       1   43  female                    art\n"
     ]
    }
   ],
   "source": [
    "# 1) Load CSVs\n",
    "interactions_df = pd.read_csv(\"user_product_interactions.csv\")  # user_id, product_id, target\n",
    "users_df = pd.read_csv(\"users.csv\")  # user_id, age, gender, hobbies\n",
    "# This \"merged_products_df\" is the result of merging products + categories in the previous step\n",
    "merged_products_df = pd.read_csv(\"merged_product_with_categories.csv\")\n",
    "# (Alternatively, merge them on the fly if you didn't save yet.)\n",
    "\n",
    "# 2) Merge interactions + users on user_id\n",
    "user_interactions = pd.merge(\n",
    "    interactions_df,\n",
    "    users_df,\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"Merged user_interactions (head):\")\n",
    "print(user_interactions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset (head):\n",
      "   user_id  product_id  target  age  gender                hobbies  \\\n",
      "0    83811  B07R9RZM7S       1   25    male     cars,gaming,sports   \n",
      "1    36049  B081H1L3SB       1   48  female   music,fashion,sports   \n",
      "2    96531  B07Y3ZX76D       0   27    male  gaming,fashion,sports   \n",
      "3    71483  B000EVU89I       0   45  female              music,art   \n",
      "4     4166  B08SHHXZDW       1   43  female                    art   \n",
      "\n",
      "                                               title  \\\n",
      "0  Women Purses and Handbags Tote Shoulder Bag To...   \n",
      "1  LA ACTIVE Non Slip Grip Ankle Boys and Girls S...   \n",
      "2                          Subterra Boarding Bag 23L   \n",
      "3       Innova 3612 Compression Tester - 4 Piece Kit   \n",
      "4  2PACK Wired Earphones with Microphone Compatib...   \n",
      "\n",
      "                                              imgUrl  \\\n",
      "0  https://m.media-amazon.com/images/I/71tEC697NQ...   \n",
      "1  https://m.media-amazon.com/images/I/81qNx9nP5U...   \n",
      "2  https://m.media-amazon.com/images/I/81PWQRVBA5...   \n",
      "3  https://m.media-amazon.com/images/I/61oxI+o+Zn...   \n",
      "4  https://m.media-amazon.com/images/I/51gX17958a...   \n",
      "\n",
      "                             productURL  stars   price  category_id  \\\n",
      "0  https://www.amazon.com/dp/B07R9RZM7S    4.3   27.98          118   \n",
      "1  https://www.amazon.com/dp/B081H1L3SB    4.7   22.49           43   \n",
      "2  https://www.amazon.com/dp/B07Y3ZX76D    4.7  146.95          108   \n",
      "3  https://www.amazon.com/dp/B000EVU89I    4.6   26.77           17   \n",
      "4  https://www.amazon.com/dp/B08SHHXZDW    3.3    4.99           71   \n",
      "\n",
      "   isBestSeller                 category_name  \n",
      "0         False              Women's Handbags  \n",
      "1         False   Baby Boys' Clothing & Shoes  \n",
      "2         False                       Luggage  \n",
      "3         False  Automotive Tools & Equipment  \n",
      "4         False          Headphones & Earbuds  \n"
     ]
    }
   ],
   "source": [
    "# 3) Merge that result with merged_products_df on product_id\n",
    "full_data = pd.merge(\n",
    "    user_interactions,\n",
    "    merged_products_df,\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"Full dataset (head):\")\n",
    "print(full_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in full_data: ['user_id', 'product_id', 'target', 'age', 'gender', 'hobbies', 'title', 'imgUrl', 'productURL', 'stars', 'price', 'category_id', 'isBestSeller', 'category_name']\n",
      "Columns in full_data afte dropping: ['user_id', 'product_id', 'target', 'age', 'gender', 'hobbies', 'title', 'stars', 'price', 'category_id', 'isBestSeller', 'category_name']\n"
     ]
    }
   ],
   "source": [
    "# 4) Inspect columns\n",
    "print(\"Columns in full_data:\", full_data.columns.tolist())\n",
    "\n",
    "# Possibly remove columns you donâ€™t need for training (e.g. 'imgUrl', etc.)\n",
    "cols_to_drop = [\"imgUrl\", \"productURL\"]  # if they exist\n",
    "for c in cols_to_drop:\n",
    "    if c in full_data.columns:\n",
    "        full_data.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 4) Inspect columns after dropping\n",
    "print(\"Columns in full_data afte dropping:\", full_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  product_id  target  age  gender        hobbies  stars  price  \\\n",
      "0     1825  B07FFCGVCS       0   40  female         sports    4.3  29.99   \n",
      "1     4013  B09KX3GPZC       1   42    male           tech    4.7  26.99   \n",
      "2     1680  B008E33K0W       1   39  female   fashion,cars    4.9   8.87   \n",
      "3     6913  B0B4X11BHQ       1   62  female    gaming,cars    4.5  19.99   \n",
      "4     3583  B00VMY21O6       0   20    male  sports,gaming    4.4  15.99   \n",
      "\n",
      "   category_id  isBestSeller                        category_name  \n",
      "0           69         False         Televisions & Video Products  \n",
      "1           97         False                         Girls' Shoes  \n",
      "2          203         False           Pumps & Plumbing Equipment  \n",
      "3          252         False  Wii U Games, Consoles & Accessories  \n",
      "4          225         False        Kids' Dress Up & Pretend Play  \n",
      "Index(['user_id', 'product_id', 'target', 'age', 'gender', 'hobbies', 'stars',\n",
      "       'price', 'category_id', 'isBestSeller', 'category_name'],\n",
      "      dtype='object')\n",
      "   user_id  product_id  target  age  gender        hobbies  stars  price  \\\n",
      "0     1825  B07FFCGVCS       0   40  female         sports    4.3  29.99   \n",
      "1     4013  B09KX3GPZC       1   42    male           tech    4.7  26.99   \n",
      "2     1680  B008E33K0W       1   39  female   fashion,cars    4.9   8.87   \n",
      "3     6913  B0B4X11BHQ       1   62  female    gaming,cars    4.5  19.99   \n",
      "4     3583  B00VMY21O6       0   20    male  sports,gaming    4.4  15.99   \n",
      "\n",
      "   category_id  isBestSeller                        category_name  \n",
      "0           69         False         Televisions & Video Products  \n",
      "1           97         False                         Girls' Shoes  \n",
      "2          203         False           Pumps & Plumbing Equipment  \n",
      "3          252         False  Wii U Games, Consoles & Accessories  \n",
      "4          225         False        Kids' Dress Up & Pretend Play  \n",
      "Index(['user_id', 'product_id', 'target', 'age', 'gender', 'hobbies', 'stars',\n",
      "       'price', 'category_id', 'isBestSeller', 'category_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"user_product_dataset.csv\")\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# # add column title (that is the product title) from amazon products cleaned csv\n",
    "# df = pd.merge(\n",
    "#     df,\n",
    "#     full_data[[\"product_id\", \"title\"]],\n",
    "#     on=\"product_id\",\n",
    "#     how=\"left\"\n",
    "# ) # it will add the title column to the df dataframe from the full_data dataframe\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id          0\n",
      "product_id       0\n",
      "target           0\n",
      "age              0\n",
      "gender           0\n",
      "hobbies          0\n",
      "stars            0\n",
      "price            0\n",
      "category_id      0\n",
      "isBestSeller     0\n",
      "category_name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Encode Categorical Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode gender\n",
    "gender_encoder = LabelEncoder()\n",
    "df['gender_encoded'] = gender_encoder.fit_transform(df['gender'].fillna('Unknown'))\n",
    "\n",
    "# Encode category_name (if you have it)\n",
    "cat_encoder = LabelEncoder()\n",
    "df['category_encoded'] = cat_encoder.fit_transform(df['category_name'].fillna('Unknown'))\n",
    "\n",
    "hobby_encoder = LabelEncoder()\n",
    "df[\"hobbies_encoded\"] = hobby_encoder.fit_transform(df[\"hobbies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hobbies_category_interaction\"] = df[\"hobbies_encoded\"] * df[\"category_encoded\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['art' 'art,books' 'art,books,cars' 'art,books,cooking'\n",
      " 'art,books,fashion' 'art,books,gaming' 'art,books,music'\n",
      " 'art,books,sports' 'art,books,tech' 'art,books,travel' 'art,cars'\n",
      " 'art,cars,books' 'art,cars,cooking' 'art,cars,fashion' 'art,cars,gaming'\n",
      " 'art,cars,music' 'art,cars,sports' 'art,cars,tech' 'art,cars,travel'\n",
      " 'art,cooking' 'art,cooking,books' 'art,cooking,cars'\n",
      " 'art,cooking,fashion' 'art,cooking,gaming' 'art,cooking,music'\n",
      " 'art,cooking,sports' 'art,cooking,tech' 'art,cooking,travel'\n",
      " 'art,fashion' 'art,fashion,books' 'art,fashion,cars'\n",
      " 'art,fashion,cooking' 'art,fashion,gaming' 'art,fashion,music'\n",
      " 'art,fashion,sports' 'art,fashion,tech' 'art,fashion,travel' 'art,gaming'\n",
      " 'art,gaming,books' 'art,gaming,cars' 'art,gaming,cooking'\n",
      " 'art,gaming,fashion' 'art,gaming,music' 'art,gaming,sports'\n",
      " 'art,gaming,tech' 'art,gaming,travel' 'art,music' 'art,music,books'\n",
      " 'art,music,cars' 'art,music,cooking' 'art,music,fashion'\n",
      " 'art,music,gaming' 'art,music,sports' 'art,music,tech' 'art,music,travel'\n",
      " 'art,sports' 'art,sports,books' 'art,sports,cars' 'art,sports,cooking'\n",
      " 'art,sports,fashion' 'art,sports,gaming' 'art,sports,music'\n",
      " 'art,sports,tech' 'art,sports,travel' 'art,tech' 'art,tech,books'\n",
      " 'art,tech,cars' 'art,tech,cooking' 'art,tech,fashion' 'art,tech,gaming'\n",
      " 'art,tech,sports' 'art,tech,travel' 'art,travel' 'art,travel,cars'\n",
      " 'art,travel,cooking' 'art,travel,fashion' 'art,travel,gaming'\n",
      " 'art,travel,music' 'art,travel,sports' 'art,travel,tech' 'books'\n",
      " 'books,art' 'books,art,cars' 'books,art,cooking' 'books,art,fashion'\n",
      " 'books,art,gaming' 'books,art,music' 'books,art,sports' 'books,art,tech'\n",
      " 'books,art,travel' 'books,cars' 'books,cars,art' 'books,cars,cooking'\n",
      " 'books,cars,fashion' 'books,cars,gaming' 'books,cars,music'\n",
      " 'books,cars,sports' 'books,cars,tech' 'books,cars,travel' 'books,cooking'\n",
      " 'books,cooking,art' 'books,cooking,cars' 'books,cooking,fashion'\n",
      " 'books,cooking,gaming' 'books,cooking,music' 'books,cooking,sports'\n",
      " 'books,cooking,tech' 'books,cooking,travel' 'books,fashion'\n",
      " 'books,fashion,art' 'books,fashion,cars' 'books,fashion,cooking'\n",
      " 'books,fashion,gaming' 'books,fashion,music' 'books,fashion,sports'\n",
      " 'books,fashion,tech' 'books,fashion,travel' 'books,gaming'\n",
      " 'books,gaming,art' 'books,gaming,cars' 'books,gaming,cooking'\n",
      " 'books,gaming,fashion' 'books,gaming,music' 'books,gaming,sports'\n",
      " 'books,gaming,tech' 'books,gaming,travel' 'books,music' 'books,music,art'\n",
      " 'books,music,cars' 'books,music,cooking' 'books,music,fashion'\n",
      " 'books,music,gaming' 'books,music,sports' 'books,music,tech'\n",
      " 'books,music,travel' 'books,sports' 'books,sports,art'\n",
      " 'books,sports,cars' 'books,sports,cooking' 'books,sports,fashion'\n",
      " 'books,sports,gaming' 'books,sports,music' 'books,sports,tech'\n",
      " 'books,sports,travel' 'books,tech' 'books,tech,art' 'books,tech,cars'\n",
      " 'books,tech,cooking' 'books,tech,fashion' 'books,tech,gaming'\n",
      " 'books,tech,music' 'books,tech,sports' 'books,tech,travel' 'books,travel'\n",
      " 'books,travel,art' 'books,travel,cars' 'books,travel,cooking'\n",
      " 'books,travel,fashion' 'books,travel,gaming' 'books,travel,music'\n",
      " 'books,travel,sports' 'books,travel,tech' 'cars' 'cars,art'\n",
      " 'cars,art,books' 'cars,art,cooking' 'cars,art,fashion' 'cars,art,gaming'\n",
      " 'cars,art,music' 'cars,art,sports' 'cars,art,tech' 'cars,art,travel'\n",
      " 'cars,books' 'cars,books,art' 'cars,books,cooking' 'cars,books,fashion'\n",
      " 'cars,books,gaming' 'cars,books,music' 'cars,books,tech'\n",
      " 'cars,books,travel' 'cars,cooking' 'cars,cooking,art'\n",
      " 'cars,cooking,books' 'cars,cooking,fashion' 'cars,cooking,gaming'\n",
      " 'cars,cooking,music' 'cars,cooking,sports' 'cars,cooking,tech'\n",
      " 'cars,cooking,travel' 'cars,fashion' 'cars,fashion,art'\n",
      " 'cars,fashion,books' 'cars,fashion,cooking' 'cars,fashion,gaming'\n",
      " 'cars,fashion,music' 'cars,fashion,sports' 'cars,fashion,tech'\n",
      " 'cars,fashion,travel' 'cars,gaming' 'cars,gaming,art' 'cars,gaming,books'\n",
      " 'cars,gaming,cooking' 'cars,gaming,fashion' 'cars,gaming,music'\n",
      " 'cars,gaming,sports' 'cars,gaming,tech' 'cars,gaming,travel' 'cars,music'\n",
      " 'cars,music,art' 'cars,music,books' 'cars,music,cooking'\n",
      " 'cars,music,fashion' 'cars,music,gaming' 'cars,music,sports'\n",
      " 'cars,music,tech' 'cars,music,travel' 'cars,sports' 'cars,sports,art'\n",
      " 'cars,sports,books' 'cars,sports,cooking' 'cars,sports,fashion'\n",
      " 'cars,sports,gaming' 'cars,sports,music' 'cars,sports,tech'\n",
      " 'cars,sports,travel' 'cars,tech' 'cars,tech,art' 'cars,tech,books'\n",
      " 'cars,tech,cooking' 'cars,tech,fashion' 'cars,tech,gaming'\n",
      " 'cars,tech,music' 'cars,tech,sports' 'cars,tech,travel' 'cars,travel'\n",
      " 'cars,travel,art' 'cars,travel,books' 'cars,travel,cooking'\n",
      " 'cars,travel,fashion' 'cars,travel,gaming' 'cars,travel,music'\n",
      " 'cars,travel,sports' 'cars,travel,tech' 'cooking' 'cooking,art'\n",
      " 'cooking,art,books' 'cooking,art,cars' 'cooking,art,fashion'\n",
      " 'cooking,art,gaming' 'cooking,art,music' 'cooking,art,sports'\n",
      " 'cooking,art,tech' 'cooking,art,travel' 'cooking,books'\n",
      " 'cooking,books,art' 'cooking,books,cars' 'cooking,books,fashion'\n",
      " 'cooking,books,gaming' 'cooking,books,music' 'cooking,books,sports'\n",
      " 'cooking,books,tech' 'cooking,books,travel' 'cooking,cars'\n",
      " 'cooking,cars,art' 'cooking,cars,books' 'cooking,cars,fashion'\n",
      " 'cooking,cars,gaming' 'cooking,cars,music' 'cooking,cars,sports'\n",
      " 'cooking,cars,tech' 'cooking,cars,travel' 'cooking,fashion'\n",
      " 'cooking,fashion,art' 'cooking,fashion,books' 'cooking,fashion,cars'\n",
      " 'cooking,fashion,gaming' 'cooking,fashion,music' 'cooking,fashion,sports'\n",
      " 'cooking,fashion,tech' 'cooking,fashion,travel' 'cooking,gaming'\n",
      " 'cooking,gaming,art' 'cooking,gaming,books' 'cooking,gaming,cars'\n",
      " 'cooking,gaming,fashion' 'cooking,gaming,music' 'cooking,gaming,sports'\n",
      " 'cooking,gaming,tech' 'cooking,gaming,travel' 'cooking,music'\n",
      " 'cooking,music,art' 'cooking,music,books' 'cooking,music,cars'\n",
      " 'cooking,music,fashion' 'cooking,music,gaming' 'cooking,music,sports'\n",
      " 'cooking,music,tech' 'cooking,music,travel' 'cooking,sports'\n",
      " 'cooking,sports,art' 'cooking,sports,books' 'cooking,sports,cars'\n",
      " 'cooking,sports,fashion' 'cooking,sports,gaming' 'cooking,sports,music'\n",
      " 'cooking,sports,tech' 'cooking,sports,travel' 'cooking,tech'\n",
      " 'cooking,tech,art' 'cooking,tech,books' 'cooking,tech,cars'\n",
      " 'cooking,tech,fashion' 'cooking,tech,gaming' 'cooking,tech,music'\n",
      " 'cooking,tech,sports' 'cooking,tech,travel' 'cooking,travel'\n",
      " 'cooking,travel,art' 'cooking,travel,books' 'cooking,travel,cars'\n",
      " 'cooking,travel,fashion' 'cooking,travel,gaming' 'cooking,travel,music'\n",
      " 'cooking,travel,sports' 'cooking,travel,tech' 'fashion' 'fashion,art'\n",
      " 'fashion,art,books' 'fashion,art,cars' 'fashion,art,cooking'\n",
      " 'fashion,art,gaming' 'fashion,art,music' 'fashion,art,sports'\n",
      " 'fashion,art,tech' 'fashion,art,travel' 'fashion,books'\n",
      " 'fashion,books,art' 'fashion,books,cars' 'fashion,books,cooking'\n",
      " 'fashion,books,gaming' 'fashion,books,music' 'fashion,books,sports'\n",
      " 'fashion,books,tech' 'fashion,books,travel' 'fashion,cars'\n",
      " 'fashion,cars,art' 'fashion,cars,books' 'fashion,cars,cooking'\n",
      " 'fashion,cars,gaming' 'fashion,cars,music' 'fashion,cars,sports'\n",
      " 'fashion,cars,tech' 'fashion,cars,travel' 'fashion,cooking'\n",
      " 'fashion,cooking,art' 'fashion,cooking,books' 'fashion,cooking,cars'\n",
      " 'fashion,cooking,gaming' 'fashion,cooking,music' 'fashion,cooking,sports'\n",
      " 'fashion,cooking,tech' 'fashion,cooking,travel' 'fashion,gaming'\n",
      " 'fashion,gaming,art' 'fashion,gaming,books' 'fashion,gaming,cars'\n",
      " 'fashion,gaming,cooking' 'fashion,gaming,music' 'fashion,gaming,sports'\n",
      " 'fashion,gaming,tech' 'fashion,gaming,travel' 'fashion,music'\n",
      " 'fashion,music,art' 'fashion,music,books' 'fashion,music,cars'\n",
      " 'fashion,music,cooking' 'fashion,music,gaming' 'fashion,music,sports'\n",
      " 'fashion,music,tech' 'fashion,music,travel' 'fashion,sports'\n",
      " 'fashion,sports,art' 'fashion,sports,books' 'fashion,sports,cars'\n",
      " 'fashion,sports,cooking' 'fashion,sports,gaming' 'fashion,sports,music'\n",
      " 'fashion,sports,tech' 'fashion,sports,travel' 'fashion,tech'\n",
      " 'fashion,tech,art' 'fashion,tech,books' 'fashion,tech,cars'\n",
      " 'fashion,tech,cooking' 'fashion,tech,gaming' 'fashion,tech,music'\n",
      " 'fashion,tech,sports' 'fashion,tech,travel' 'fashion,travel'\n",
      " 'fashion,travel,art' 'fashion,travel,books' 'fashion,travel,cars'\n",
      " 'fashion,travel,cooking' 'fashion,travel,gaming' 'fashion,travel,music'\n",
      " 'fashion,travel,sports' 'fashion,travel,tech' 'gaming' 'gaming,art'\n",
      " 'gaming,art,books' 'gaming,art,cars' 'gaming,art,cooking'\n",
      " 'gaming,art,fashion' 'gaming,art,music' 'gaming,art,sports'\n",
      " 'gaming,art,tech' 'gaming,art,travel' 'gaming,books' 'gaming,books,art'\n",
      " 'gaming,books,cars' 'gaming,books,cooking' 'gaming,books,fashion'\n",
      " 'gaming,books,music' 'gaming,books,sports' 'gaming,books,tech'\n",
      " 'gaming,books,travel' 'gaming,cars' 'gaming,cars,art' 'gaming,cars,books'\n",
      " 'gaming,cars,cooking' 'gaming,cars,fashion' 'gaming,cars,music'\n",
      " 'gaming,cars,sports' 'gaming,cars,tech' 'gaming,cars,travel'\n",
      " 'gaming,cooking' 'gaming,cooking,art' 'gaming,cooking,books'\n",
      " 'gaming,cooking,cars' 'gaming,cooking,fashion' 'gaming,cooking,music'\n",
      " 'gaming,cooking,sports' 'gaming,cooking,tech' 'gaming,cooking,travel'\n",
      " 'gaming,fashion' 'gaming,fashion,art' 'gaming,fashion,books'\n",
      " 'gaming,fashion,cars' 'gaming,fashion,cooking' 'gaming,fashion,music'\n",
      " 'gaming,fashion,sports' 'gaming,fashion,tech' 'gaming,fashion,travel'\n",
      " 'gaming,music' 'gaming,music,art' 'gaming,music,books'\n",
      " 'gaming,music,cars' 'gaming,music,cooking' 'gaming,music,fashion'\n",
      " 'gaming,music,sports' 'gaming,music,tech' 'gaming,music,travel'\n",
      " 'gaming,sports' 'gaming,sports,art' 'gaming,sports,books'\n",
      " 'gaming,sports,cars' 'gaming,sports,cooking' 'gaming,sports,fashion'\n",
      " 'gaming,sports,music' 'gaming,sports,tech' 'gaming,sports,travel'\n",
      " 'gaming,tech' 'gaming,tech,art' 'gaming,tech,books' 'gaming,tech,cars'\n",
      " 'gaming,tech,cooking' 'gaming,tech,music' 'gaming,tech,sports'\n",
      " 'gaming,tech,travel' 'gaming,travel' 'gaming,travel,art'\n",
      " 'gaming,travel,books' 'gaming,travel,cars' 'gaming,travel,cooking'\n",
      " 'gaming,travel,fashion' 'gaming,travel,music' 'gaming,travel,sports'\n",
      " 'gaming,travel,tech' 'music' 'music,art' 'music,art,books'\n",
      " 'music,art,cars' 'music,art,cooking' 'music,art,fashion'\n",
      " 'music,art,gaming' 'music,art,sports' 'music,art,tech' 'music,art,travel'\n",
      " 'music,books' 'music,books,art' 'music,books,cars' 'music,books,cooking'\n",
      " 'music,books,fashion' 'music,books,gaming' 'music,books,sports'\n",
      " 'music,books,tech' 'music,books,travel' 'music,cars' 'music,cars,art'\n",
      " 'music,cars,books' 'music,cars,cooking' 'music,cars,fashion'\n",
      " 'music,cars,gaming' 'music,cars,sports' 'music,cars,tech'\n",
      " 'music,cars,travel' 'music,cooking' 'music,cooking,art'\n",
      " 'music,cooking,books' 'music,cooking,cars' 'music,cooking,fashion'\n",
      " 'music,cooking,gaming' 'music,cooking,sports' 'music,cooking,tech'\n",
      " 'music,cooking,travel' 'music,fashion' 'music,fashion,art'\n",
      " 'music,fashion,books' 'music,fashion,cars' 'music,fashion,cooking'\n",
      " 'music,fashion,gaming' 'music,fashion,sports' 'music,fashion,tech'\n",
      " 'music,fashion,travel' 'music,gaming' 'music,gaming,art'\n",
      " 'music,gaming,books' 'music,gaming,cars' 'music,gaming,cooking'\n",
      " 'music,gaming,fashion' 'music,gaming,sports' 'music,gaming,tech'\n",
      " 'music,gaming,travel' 'music,sports' 'music,sports,art'\n",
      " 'music,sports,books' 'music,sports,cars' 'music,sports,cooking'\n",
      " 'music,sports,fashion' 'music,sports,gaming' 'music,sports,tech'\n",
      " 'music,sports,travel' 'music,tech' 'music,tech,art' 'music,tech,books'\n",
      " 'music,tech,cars' 'music,tech,cooking' 'music,tech,fashion'\n",
      " 'music,tech,gaming' 'music,tech,sports' 'music,tech,travel'\n",
      " 'music,travel' 'music,travel,art' 'music,travel,books'\n",
      " 'music,travel,cars' 'music,travel,cooking' 'music,travel,fashion'\n",
      " 'music,travel,gaming' 'music,travel,sports' 'music,travel,tech' 'sports'\n",
      " 'sports,art' 'sports,art,books' 'sports,art,cars' 'sports,art,cooking'\n",
      " 'sports,art,fashion' 'sports,art,gaming' 'sports,art,music'\n",
      " 'sports,art,tech' 'sports,art,travel' 'sports,books' 'sports,books,art'\n",
      " 'sports,books,cars' 'sports,books,cooking' 'sports,books,fashion'\n",
      " 'sports,books,gaming' 'sports,books,music' 'sports,books,tech'\n",
      " 'sports,books,travel' 'sports,cars' 'sports,cars,art' 'sports,cars,books'\n",
      " 'sports,cars,cooking' 'sports,cars,fashion' 'sports,cars,gaming'\n",
      " 'sports,cars,music' 'sports,cars,tech' 'sports,cars,travel'\n",
      " 'sports,cooking' 'sports,cooking,art' 'sports,cooking,books'\n",
      " 'sports,cooking,cars' 'sports,cooking,fashion' 'sports,cooking,gaming'\n",
      " 'sports,cooking,music' 'sports,cooking,tech' 'sports,cooking,travel'\n",
      " 'sports,fashion' 'sports,fashion,art' 'sports,fashion,books'\n",
      " 'sports,fashion,cars' 'sports,fashion,cooking' 'sports,fashion,gaming'\n",
      " 'sports,fashion,music' 'sports,fashion,tech' 'sports,fashion,travel'\n",
      " 'sports,gaming' 'sports,gaming,art' 'sports,gaming,books'\n",
      " 'sports,gaming,cars' 'sports,gaming,cooking' 'sports,gaming,fashion'\n",
      " 'sports,gaming,music' 'sports,gaming,tech' 'sports,gaming,travel'\n",
      " 'sports,music' 'sports,music,art' 'sports,music,books'\n",
      " 'sports,music,cars' 'sports,music,cooking' 'sports,music,fashion'\n",
      " 'sports,music,gaming' 'sports,music,tech' 'sports,music,travel'\n",
      " 'sports,tech' 'sports,tech,art' 'sports,tech,books' 'sports,tech,cars'\n",
      " 'sports,tech,cooking' 'sports,tech,fashion' 'sports,tech,gaming'\n",
      " 'sports,tech,music' 'sports,tech,travel' 'sports,travel'\n",
      " 'sports,travel,art' 'sports,travel,books' 'sports,travel,cars'\n",
      " 'sports,travel,cooking' 'sports,travel,fashion' 'sports,travel,gaming'\n",
      " 'sports,travel,music' 'sports,travel,tech' 'tech' 'tech,art'\n",
      " 'tech,art,books' 'tech,art,cars' 'tech,art,cooking' 'tech,art,fashion'\n",
      " 'tech,art,gaming' 'tech,art,music' 'tech,art,sports' 'tech,art,travel'\n",
      " 'tech,books' 'tech,books,art' 'tech,books,cars' 'tech,books,cooking'\n",
      " 'tech,books,fashion' 'tech,books,gaming' 'tech,books,music'\n",
      " 'tech,books,sports' 'tech,books,travel' 'tech,cars' 'tech,cars,art'\n",
      " 'tech,cars,books' 'tech,cars,cooking' 'tech,cars,fashion'\n",
      " 'tech,cars,gaming' 'tech,cars,music' 'tech,cars,sports'\n",
      " 'tech,cars,travel' 'tech,cooking' 'tech,cooking,art' 'tech,cooking,books'\n",
      " 'tech,cooking,cars' 'tech,cooking,fashion' 'tech,cooking,gaming'\n",
      " 'tech,cooking,music' 'tech,cooking,sports' 'tech,cooking,travel'\n",
      " 'tech,fashion' 'tech,fashion,art' 'tech,fashion,books'\n",
      " 'tech,fashion,cars' 'tech,fashion,cooking' 'tech,fashion,gaming'\n",
      " 'tech,fashion,music' 'tech,fashion,sports' 'tech,fashion,travel'\n",
      " 'tech,gaming' 'tech,gaming,art' 'tech,gaming,books' 'tech,gaming,cars'\n",
      " 'tech,gaming,cooking' 'tech,gaming,fashion' 'tech,gaming,music'\n",
      " 'tech,gaming,sports' 'tech,gaming,travel' 'tech,music' 'tech,music,art'\n",
      " 'tech,music,books' 'tech,music,cars' 'tech,music,cooking'\n",
      " 'tech,music,fashion' 'tech,music,gaming' 'tech,music,sports'\n",
      " 'tech,music,travel' 'tech,sports' 'tech,sports,art' 'tech,sports,books'\n",
      " 'tech,sports,cars' 'tech,sports,cooking' 'tech,sports,fashion'\n",
      " 'tech,sports,gaming' 'tech,sports,music' 'tech,sports,travel'\n",
      " 'tech,travel' 'tech,travel,art' 'tech,travel,books' 'tech,travel,cars'\n",
      " 'tech,travel,cooking' 'tech,travel,fashion' 'tech,travel,gaming'\n",
      " 'tech,travel,music' 'tech,travel,sports' 'travel' 'travel,art'\n",
      " 'travel,art,books' 'travel,art,cars' 'travel,art,cooking'\n",
      " 'travel,art,fashion' 'travel,art,gaming' 'travel,art,music'\n",
      " 'travel,art,sports' 'travel,art,tech' 'travel,books' 'travel,books,art'\n",
      " 'travel,books,cars' 'travel,books,cooking' 'travel,books,fashion'\n",
      " 'travel,books,gaming' 'travel,books,music' 'travel,books,sports'\n",
      " 'travel,books,tech' 'travel,cars' 'travel,cars,art' 'travel,cars,books'\n",
      " 'travel,cars,cooking' 'travel,cars,fashion' 'travel,cars,gaming'\n",
      " 'travel,cars,music' 'travel,cars,sports' 'travel,cars,tech'\n",
      " 'travel,cooking' 'travel,cooking,art' 'travel,cooking,books'\n",
      " 'travel,cooking,cars' 'travel,cooking,fashion' 'travel,cooking,gaming'\n",
      " 'travel,cooking,music' 'travel,cooking,sports' 'travel,cooking,tech'\n",
      " 'travel,fashion' 'travel,fashion,art' 'travel,fashion,books'\n",
      " 'travel,fashion,cars' 'travel,fashion,cooking' 'travel,fashion,gaming'\n",
      " 'travel,fashion,music' 'travel,fashion,sports' 'travel,fashion,tech'\n",
      " 'travel,gaming' 'travel,gaming,art' 'travel,gaming,books'\n",
      " 'travel,gaming,cars' 'travel,gaming,cooking' 'travel,gaming,fashion'\n",
      " 'travel,gaming,music' 'travel,gaming,sports' 'travel,gaming,tech'\n",
      " 'travel,music' 'travel,music,art' 'travel,music,books'\n",
      " 'travel,music,cars' 'travel,music,cooking' 'travel,music,fashion'\n",
      " 'travel,music,gaming' 'travel,music,sports' 'travel,music,tech'\n",
      " 'travel,sports' 'travel,sports,art' 'travel,sports,books'\n",
      " 'travel,sports,cars' 'travel,sports,cooking' 'travel,sports,fashion'\n",
      " 'travel,sports,gaming' 'travel,sports,music' 'travel,sports,tech'\n",
      " 'travel,tech' 'travel,tech,art' 'travel,tech,books' 'travel,tech,cars'\n",
      " 'travel,tech,cooking' 'travel,tech,fashion' 'travel,tech,gaming'\n",
      " 'travel,tech,music' 'travel,tech,sports']\n"
     ]
    }
   ],
   "source": [
    "# inspect unique values of hobbies\n",
    "#print(df['hobbies'].unique())\n",
    "print(hobby_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numeric fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['price_scaled'] = scaler.fit_transform(df[['price']])\n",
    "df['stars_scaled'] = scaler.fit_transform(df[['stars']])\n",
    "\n",
    "# Keep the scaled columns only\n",
    "df.drop(columns=['price', 'stars'], inplace=True)\n",
    "\n",
    "# from full_data dataframe, add column 'title' to the df.\n",
    "df = df.merge(full_data[['product_id', 'title']], \n",
    "              on='product_id', \n",
    "              how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                             0\n",
      "product_id                          0\n",
      "target                              0\n",
      "age                                 0\n",
      "gender                              0\n",
      "hobbies                             0\n",
      "category_id                         0\n",
      "isBestSeller                        0\n",
      "category_name                       0\n",
      "gender_encoded                      0\n",
      "category_encoded                    0\n",
      "hobbies_encoded                     0\n",
      "hobbies_category_interaction        0\n",
      "price_scaled                        0\n",
      "stars_scaled                        0\n",
      "title                           46907\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to final_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"final_dataset.csv\", index=False)\n",
    "print(\"Final dataset saved to final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra experiments: Interaction Features between scaled price and category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'age',\n",
    "    'gender_encoded',\n",
    "    'hobbies_encoded',\n",
    "    'price_scaled',\n",
    "    'stars_scaled',\n",
    "    'category_encoded',\n",
    "    'isBestSeller',\n",
    "    'hobbies_category_interaction'\n",
    "\n",
    "]\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Convert 'isBestSeller' to int if needed\n",
    "X['isBestSeller'] = X['isBestSeller'].astype(int)\n",
    "\n",
    "y = df['target']  # 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category_encoder.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(hobby_encoder, \"hobby_encoder.pkl\")\n",
    "joblib.dump(cat_encoder, \"category_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    121980\n",
      "1     52812\n",
      "2     59512\n",
      "3    100962\n",
      "4     65912\n",
      "5      9492\n",
      "6     66748\n",
      "7     64904\n",
      "8     31236\n",
      "9     55062\n",
      "Name: hobbies_category_interaction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"hobbies_category_interaction\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17776, number of negative: 65190\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 82966, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214256 -> initscore=-1.299457\n",
      "[LightGBM] [Info] Start training from score -1.299457\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    # You can tune parameters like num_leaves, max_depth, etc.\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87     16124\n",
      "           1       0.58      0.00      0.01      4618\n",
      "\n",
      "    accuracy                           0.78     20742\n",
      "   macro avg       0.68      0.50      0.44     20742\n",
      "weighted avg       0.73      0.78      0.68     20742\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16108    16]\n",
      " [ 4596    22]]\n",
      "ROC AUC Score: 0.6928025268439838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11850, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 55310, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214247 -> initscore=-1.299513\n",
      "[LightGBM] [Info] Start training from score -1.299513\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 11851, number of negative: 43460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 55311, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214261 -> initscore=-1.299429\n",
      "[LightGBM] [Info] Start training from score -1.299429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 17776, number of negative: 65190\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 82966, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214256 -> initscore=-1.299457\n",
      "[LightGBM] [Info] Start training from score -1.299457\n",
      "Best parameters: {'colsample_bytree': 0.8, 'max_depth': -1, 'min_data_in_leaf': 50, 'num_leaves': 31}\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "ROC AUC of best model: 0.6923880729349705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 127],\n",
    "    'max_depth': [-1, 20],\n",
    "    'min_data_in_leaf': [5, 50],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=LGBMClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_best_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC of best model:\", roc_auc_score(y_test, y_pred_best_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 17776, number of negative: 65190\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 82966, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82     16124\n",
      "           1       0.36      0.37      0.36      4618\n",
      "\n",
      "    accuracy                           0.71     20742\n",
      "   macro avg       0.59      0.59      0.59     20742\n",
      "weighted avg       0.72      0.71      0.72     20742\n",
      "\n",
      "Confusion Matrix:\n",
      " [[13120  3004]\n",
      " [ 2919  1699]]\n",
      "ROC AUC Score: 0.673029581591518\n"
     ]
    }
   ],
   "source": [
    "# Use your best hyperparameters\n",
    "best_params = {\n",
    "    'num_leaves': 31,\n",
    "    'n_estimators': 10000,\n",
    "    'max_depth': -1,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42  # keep the same seed for reproducibility\n",
    "}\n",
    "\n",
    "# Instantiate the final model\n",
    "final_model = LGBMClassifier(**best_params)\n",
    "\n",
    "# Fit on the entire training set (or the train split) again\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on your test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved as 'final_model.pkl'!\n"
     ]
    }
   ],
   "source": [
    "# save the last model\n",
    "joblib.dump(final_model, \"final_model.pkl\")\n",
    "print(\"Final model saved as 'final_model.pkl'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in model: ['age', 'gender_encoded', 'hobbies_encoded', 'price_scaled', 'stars_scaled', 'category_encoded', 'isBestSeller', 'hobbies_category_interaction']\n"
     ]
    }
   ],
   "source": [
    "print(\"Features in model:\", final_model.feature_name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidates[[\"age\", \"gender_encoded\", \"hobbies_encoded\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model feature names: ['age', 'gender_encoded', 'hobbies_encoded', 'price_scaled', 'stars_scaled', 'category_encoded', 'isBestSeller', 'hobbies_category_interaction']\n"
     ]
    }
   ],
   "source": [
    "print(\"Model feature names:\", model.booster_.feature_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "Train predictions distribution:\n",
      "0    63287\n",
      "1    19679\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# inspect model.predict(X_train) distribution\n",
    "train_pred = final_model.predict(X_train)\n",
    "print(\"Train predictions distribution:\")\n",
    "print(pd.Series(train_pred).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances = final_model.feature_importances_\n",
    "plt.barh(feature_cols, feature_importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_encoded\n",
      "78     2016\n",
      "34     1702\n",
      "220    1508\n",
      "240    1396\n",
      "79     1337\n",
      "       ... \n",
      "201       2\n",
      "150       2\n",
      "202       2\n",
      "192       1\n",
      "74        1\n",
      "Name: count, Length: 248, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"category_encoded\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         asin                                              title  \\\n",
      "0  B014TMV5YE  Sion Softside Expandable Roller Luggage, Black...   \n",
      "1  B07GDLCQXV  Luggage Sets Expandable PC+ABS Durable Suitcas...   \n",
      "2  B07XSCCZYG  Platinum Elite Softside Expandable Checked Lug...   \n",
      "3  B08MVFKGJM  Freeform Hardside Expandable with Double Spinn...   \n",
      "4  B01DJLKZBA  Winfield 2 Hardside Expandable Luggage with Sp...   \n",
      "\n",
      "                                              imgUrl  \\\n",
      "0  https://m.media-amazon.com/images/I/815dLQKYIY...   \n",
      "1  https://m.media-amazon.com/images/I/81bQlm7vf6...   \n",
      "2  https://m.media-amazon.com/images/I/71EA35zvJB...   \n",
      "3  https://m.media-amazon.com/images/I/91k6NYLQyI...   \n",
      "4  https://m.media-amazon.com/images/I/61NJoaZcP9...   \n",
      "\n",
      "                             productURL  stars   price  category_id  \\\n",
      "0  https://www.amazon.com/dp/B014TMV5YE    4.5  139.99          104   \n",
      "1  https://www.amazon.com/dp/B07GDLCQXV    4.5  169.99          104   \n",
      "2  https://www.amazon.com/dp/B07XSCCZYG    4.6  365.49          104   \n",
      "3  https://www.amazon.com/dp/B08MVFKGJM    4.6  291.59          104   \n",
      "4  https://www.amazon.com/dp/B01DJLKZBA    4.5  174.99          104   \n",
      "\n",
      "   isBestSeller  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3         False  \n",
      "4         False  \n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"amazon_products_cleaned.csv\")\n",
    "\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  product_id  target  age  gender        hobbies  category_id  \\\n",
      "0     1825  B07FFCGVCS       0   40  female         sports           69   \n",
      "1     4013  B09KX3GPZC       1   42    male           tech           97   \n",
      "2     1680  B008E33K0W       1   39  female   fashion,cars          203   \n",
      "3     6913  B0B4X11BHQ       1   62  female    gaming,cars          252   \n",
      "4     3583  B00VMY21O6       0   20    male  sports,gaming          225   \n",
      "\n",
      "   isBestSeller                        category_name  gender_encoded  \\\n",
      "0         False         Televisions & Video Products               0   \n",
      "1         False                         Girls' Shoes               1   \n",
      "2         False           Pumps & Plumbing Equipment               0   \n",
      "3         False  Wii U Games, Consoles & Accessories               0   \n",
      "4         False        Kids' Dress Up & Pretend Play               1   \n",
      "\n",
      "   category_encoded  hobbies_encoded  hobbies_category_interaction  \\\n",
      "0               214              570                        121980   \n",
      "1                81              652                         52812   \n",
      "2               173              344                         59512   \n",
      "3               237              426                        100962   \n",
      "4               107              616                         65912   \n",
      "\n",
      "   price_scaled  stars_scaled  \n",
      "0      0.003997          0.86  \n",
      "1      0.003597          0.94  \n",
      "2      0.001181          0.98  \n",
      "3      0.002664          0.90  \n",
      "4      0.002131          0.88  \n"
     ]
    }
   ],
   "source": [
    "test_2 = pd.read_csv(\"final_dataset.csv\")   \n",
    "print(test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
